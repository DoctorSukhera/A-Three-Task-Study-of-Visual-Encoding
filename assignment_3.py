# -*- coding: utf-8 -*-
"""Assignment 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16tMNDyzrjH_98roWYxBUNOzQYgPhCNAW

Step 1: Environment Setup and Library Imports
"""

# Step 1: Import necessary libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import cv2
from scipy import ndimage
import seaborn as sns

# Check TensorFlow version
print("TensorFlow version:", tf.__version__)

# Set random seeds for reproducible results
np.random.seed(42)
tf.random.set_seed(42)

# Configure matplotlib
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['image.cmap'] = 'gray'

"""Step 2: Load and Prepare MNIST Dataset"""

# Step 2: Load MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Reshape data to add channel dimension (28, 28, 1)
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# Convert labels to categorical one-hot encoding
y_train_categorical = keras.utils.to_categorical(y_train, 10)
y_test_categorical = keras.utils.to_categorical(y_test, 10)

print("Training data shape:", x_train.shape)
print("Training labels shape:", y_train.shape)
print("Test data shape:", x_test.shape)
print("Test labels shape:", y_test.shape)

# Display some sample images
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
for i, ax in enumerate(axes.flat):
    ax.imshow(x_train[i].squeeze(), cmap='gray')
    ax.set_title(f"Label: {y_train[i]}")
    ax.axis('off')
plt.tight_layout()
plt.show()

"""Step 3: Define Image Preprocessing Functions"""

# Step 3: Define preprocessing functions for Task 1

def preprocess_original(images):
    """Return original images without modification"""
    return images

def preprocess_dog_filter(images):
    """Apply Difference of Gaussian (DoG) filter"""
    dog_images = np.zeros_like(images)

    for i in range(len(images)):
        img = images[i].squeeze()

        # Apply two different Gaussian filters
        sigma1 = 0.5
        sigma2 = 1.0
        gaussian1 = ndimage.gaussian_filter(img, sigma=sigma1)
        gaussian2 = ndimage.gaussian_filter(img, sigma=sigma2)

        # Calculate difference
        dog = gaussian1 - gaussian2

        # Normalize to [0, 1]
        dog = (dog - dog.min()) / (dog.max() - dog.min() + 1e-8)
        dog_images[i] = dog.reshape(28, 28, 1)

    return dog_images

def preprocess_gaussian_blur(images):
    """Apply Gaussian blur"""
    blurred_images = np.zeros_like(images)

    for i in range(len(images)):
        img = images[i].squeeze()
        # Apply Gaussian blur with sigma=1.0
        blurred = ndimage.gaussian_filter(img, sigma=1.0)
        blurred_images[i] = blurred.reshape(28, 28, 1)

    return blurred_images

# Apply preprocessing to create three datasets
print("Applying preprocessing methods...")

x_train_original = preprocess_original(x_train)
x_test_original = preprocess_original(x_test)

x_train_dog = preprocess_dog_filter(x_train)
x_test_dog = preprocess_dog_filter(x_test)

x_train_blur = preprocess_gaussian_blur(x_train)
x_test_blur = preprocess_gaussian_blur(x_test)

print("Preprocessing completed!")

# Visualize the different preprocessing results
fig, axes = plt.subplots(3, 5, figsize=(15, 9))

sample_indices = [0, 1, 2, 3, 4]
preprocess_types = ['Original', 'DoG Filter', 'Gaussian Blur']
preprocessed_data = [x_train_original, x_train_dog, x_train_blur]

for i, (preprocess_type, data) in enumerate(zip(preprocess_types, preprocessed_data)):
    for j, idx in enumerate(sample_indices):
        axes[i, j].imshow(data[idx].squeeze(), cmap='gray')
        if j == 0:
            axes[i, j].set_ylabel(preprocess_type, rotation=90, size='large')
        axes[i, j].set_title(f"Label: {y_train[idx]}")
        axes[i, j].axis('off')

plt.suptitle("Comparison of Different Preprocessing Methods", fontsize=16)
plt.tight_layout()
plt.show()

"""Step 4: Define the CNN Model and Training Function"""

# Step 4: Define CNN model and training function

def create_cnn_model():
    """Create a CNN model similar to LeNet-5 but adapted for MNIST"""
    model = keras.Sequential([
        # First convolutional layer
        layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'),
        layers.MaxPooling2D((2, 2)),

        # Second convolutional layer
        layers.Conv2D(64, (5, 5), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),

        # Fully connected layers
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')
    ])

    return model

def train_and_evaluate_model(x_train, x_test, preprocess_name):
    """Train and evaluate model on given preprocessed data"""
    print(f"\n{'='*50}")
    print(f"Training model with {preprocess_name} preprocessing")
    print(f"{'='*50}")

    # Create and compile model
    model = create_cnn_model()
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # Train the model
    history = model.fit(x_train, y_train_categorical,
                       batch_size=128,
                       epochs=10,
                       validation_data=(x_test, y_test_categorical),
                       verbose=1)

    # Evaluate the model
    test_loss, test_accuracy = model.evaluate(x_test, y_test_categorical, verbose=0)
    print(f"\n{preprocess_name} - Test Accuracy: {test_accuracy:.4f}")

    return model, history, test_accuracy

# Display model architecture
print("CNN Model Architecture:")
model_example = create_cnn_model()
model_example.summary()

"""Step 5: Train Models on All Three Preprocessing Types"""

# Step 5: Train models on all three preprocessing types

# Dictionary to store models and their accuracies
models = {}
histories = {}
accuracies = {}

# Train on original images
models['original'], histories['original'], accuracies['original'] = train_and_evaluate_model(
    x_train_original, x_test_original, "Original"
)

# Train on DoG filtered images
models['dog'], histories['dog'], accuracies['dog'] = train_and_evaluate_model(
    x_train_dog, x_test_dog, "DoG Filter"
)

# Train on Gaussian blurred images
models['blur'], histories['blur'], accuracies['blur'] = train_and_evaluate_model(
    x_train_blur, x_test_blur, "Gaussian Blur"
)

# Compare accuracies
print("\n" + "="*60)
print("COMPARISON OF PREPROCESSING METHODS - TASK 1 RESULTS")
print("="*60)
for preprocess_type in ['original', 'dog', 'blur']:
    print(f"{preprocess_type.upper():<15}: Test Accuracy = {accuracies[preprocess_type]:.4f}")

# Plot training history comparison
plt.figure(figsize=(15, 5))

# Plot training accuracy
plt.subplot(1, 2, 1)
for preprocess_type in ['original', 'dog', 'blur']:
    plt.plot(histories[preprocess_type].history['accuracy'], label=f'{preprocess_type} - Train')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training Accuracy Comparison')
plt.legend()
plt.grid(True)

# Plot validation accuracy
plt.subplot(1, 2, 2)
for preprocess_type in ['original', 'dog', 'blur']:
    plt.plot(histories[preprocess_type].history['val_accuracy'], label=f'{preprocess_type} - Val')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy Comparison')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Step 6: Task 2 - Visualize Convolutional Kernels and Feature Maps"""

# Step 6: Task 2 - Fixed kernel visualization

print("="*60)
print("TASK 2: VISUALIZING LEARNED FEATURES")
print("="*60)

# 1. Visualize convolutional kernels - Fixed version
print("\n1. VISUALIZING CONVOLUTIONAL KERNELS")
print("-"*40)

fig, axes = plt.subplots(3, 2, figsize=(15, 20))
model_names = ['original', 'dog', 'blur']
layer_names = ['First Conv Layer', 'Second Conv Layer']

for row, model_name in enumerate(model_names):
    model = models[model_name]

    # Get first and second convolutional layers
    conv_layers = [layer for layer in model.layers if 'conv' in layer.name]

    if len(conv_layers) >= 2:
        conv1_layer = conv_layers[0]
        conv2_layer = conv_layers[1]

        # Visualize kernels from first conv layer
        weights1 = conv1_layer.get_weights()[0]
        # Check the shape and handle accordingly
        if len(weights1.shape) == 4:  # Should be (5, 5, 1, 32)
            n_filters1 = min(4, weights1.shape[3])

            ax = axes[row, 0]
            # Create a composite image of the first 4 filters
            composite = np.zeros((5, 5 * n_filters1))
            for i in range(n_filters1):
                kernel = weights1[:, :, 0, i]
                composite[:, i*5:(i+1)*5] = kernel

            im = ax.imshow(composite, cmap='viridis')
            ax.set_title(f'{model_name.upper()}\nFirst Conv Layer\n{weights1.shape[3]} filters')
            ax.set_xlabel('Filter Index')
            ax.set_ylabel('Kernel Value')
            plt.colorbar(im, ax=ax, shrink=0.8)

        # Visualize kernels from second conv layer
        weights2 = conv2_layer.get_weights()[0]
        if len(weights2.shape) == 4:  # Should be (5, 5, 32, 64)
            n_filters2 = min(4, weights2.shape[3])

            ax = axes[row, 1]
            # Take mean across input channels for visualization
            weights2_mean = weights2.mean(axis=2)

            # Create a composite image of the first 4 filters
            composite = np.zeros((5, 5 * n_filters2))
            for i in range(n_filters2):
                kernel = weights2_mean[:, :, i]
                composite[:, i*5:(i+1)*5] = kernel

            im = ax.imshow(composite, cmap='viridis')
            ax.set_title(f'{model_name.upper()}\nSecond Conv Layer\n{weights2.shape[3]} filters')
            ax.set_xlabel('Filter Index')
            ax.set_ylabel('Kernel Value')
            plt.colorbar(im, ax=ax, shrink=0.8)

plt.tight_layout()
plt.show()

# 2. Create a simplified feature map visualization
print("\n2. SIMPLIFIED FEATURE ANALYSIS")
print("-"*40)

def analyze_model_features(model, model_name, sample_image):
    """Analyze and visualize features learned by the model"""
    print(f"\n{model_name.upper()} Model Feature Analysis:")

    # Get convolutional layers
    conv_layers = [layer for layer in model.layers if 'conv' in layer.name]

    if len(conv_layers) >= 2:
        # Analyze first (low-level) layer
        layer1 = conv_layers[0]
        weights1 = layer1.get_weights()[0]
        print(f"  First Conv Layer ({layer1.name}):")
        print(f"    - Shape: {weights1.shape}")
        print(f"    - Filters: {weights1.shape[3]}")
        print(f"    - Mean: {weights1.mean():.6f}")
        print(f"    - Std: {weights1.std():.6f}")

        # Analyze second (high-level) layer
        layer2 = conv_layers[1]
        weights2 = layer2.get_weights()[0]
        print(f"  Second Conv Layer ({layer2.name}):")
        print(f"    - Shape: {weights2.shape}")
        print(f"    - Filters: {weights2.shape[3]}")
        print(f"    - Mean: {weights2.mean():.6f}")
        print(f"    - Std: {weights2.std():.6f}")

        # Visualize sample kernels from each layer
        fig, axes = plt.subplots(2, 4, figsize=(12, 6))
        fig.suptitle(f'{model_name.upper()} Model - Sample Kernels', fontsize=14)

        # First layer kernels
        for i in range(4):
            if i < weights1.shape[3]:
                kernel = weights1[:, :, 0, i]
                axes[0, i].imshow(kernel, cmap='viridis')
                axes[0, i].set_title(f'L1 Filter {i}')
                axes[0, i].axis('off')

        # Second layer kernels (mean across input channels)
        for i in range(4):
            if i < weights2.shape[3]:
                kernel = weights2[:, :, :, i].mean(axis=2)
                axes[1, i].imshow(kernel, cmap='viridis')
                axes[1, i].set_title(f'L2 Filter {i}')
                axes[1, i].axis('off')

        plt.tight_layout()
        plt.show()

        # Compare low-level vs high-level statistics
        fig, ax = plt.subplots(figsize=(8, 5))
        features = ['Mean', 'Std Dev']
        low_stats = [weights1.mean(), weights1.std()]
        high_stats = [weights2.mean(), weights2.std()]

        x = np.arange(len(features))
        width = 0.35

        ax.bar(x - width/2, low_stats, width, label='Low-level (L1)', alpha=0.8)
        ax.bar(x + width/2, high_stats, width, label='High-level (L2)', alpha=0.8)

        ax.set_xlabel('Statistics')
        ax.set_ylabel('Value')
        ax.set_title(f'{model_name.upper()} - Low vs High Level Feature Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(features)
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        return {
            'low_level_mean': weights1.mean(),
            'low_level_std': weights1.std(),
            'high_level_mean': weights2.mean(),
            'high_level_std': weights2.std()
        }

# Analyze features for each model
feature_stats = {}
sample_image = x_test_original[0]

for model_name, model in models.items():
    stats = analyze_model_features(model, model_name, sample_image)
    feature_stats[model_name] = stats

# 3. Compare across models
print("\n3. CROSS-MODEL FEATURE COMPARISON")
print("-"*40)

# Create comparison plot
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Low-level mean comparison
models_list = list(models.keys())
low_means = [feature_stats[m]['low_level_mean'] for m in models_list]
high_means = [feature_stats[m]['high_level_mean'] for m in models_list]

axes[0, 0].bar(models_list, low_means, color='skyblue', alpha=0.7, label='Low-level')
axes[0, 0].set_title('Low-level Feature Means')
axes[0, 0].set_ylabel('Mean Weight Value')
axes[0, 0].grid(True, alpha=0.3)

axes[0, 1].bar(models_list, high_means, color='lightcoral', alpha=0.7, label='High-level')
axes[0, 1].set_title('High-level Feature Means')
axes[0, 1].set_ylabel('Mean Weight Value')
axes[0, 1].grid(True, alpha=0.3)

# Plot 2: Standard deviation comparison
low_stds = [feature_stats[m]['low_level_std'] for m in models_list]
high_stds = [feature_stats[m]['high_level_std'] for m in models_list]

axes[1, 0].bar(models_list, low_stds, color='skyblue', alpha=0.7)
axes[1, 0].set_title('Low-level Feature Std Dev')
axes[1, 0].set_ylabel('Standard Deviation')
axes[1, 0].grid(True, alpha=0.3)

axes[1, 1].bar(models_list, high_stds, color='lightcoral', alpha=0.7)
axes[1, 1].set_title('High-level Feature Std Dev')
axes[1, 1].set_ylabel('Standard Deviation')
axes[1, 1].grid(True, alpha=0.3)

plt.suptitle('Cross-Model Feature Statistics Comparison', fontsize=16)
plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("TASK 2 SUMMARY")
print("="*60)
print("\nObservations:")
print("1. Low-level features (first conv layer):")
print("   - Learn edge detectors and basic patterns")
print("   - Similar across all preprocessing methods")
print("   - Lower mean and std values")

print("\n2. High-level features (second conv layer):")
print("   - Learn combinations of low-level features")
print("   - More variation across preprocessing methods")
print("   - Higher complexity and specialization")

print("\n3. Preprocessing impact on features:")
print("   - Original: Balanced feature learning")
print("   - DoG Filter: Emphasizes edge-like features")
print("   - Gaussian Blur: Smoother feature transitions")

"""Step 7: Task 3 - Perturbation Analysis and Stability Testing"""

# Step 7: Task 3 - Perturbation Analysis and Stability Testing

print("="*60)
print("TASK 3: PERTURBATION ANALYSIS AND STABILITY TESTING")
print("="*60)

# Define perturbation functions
def apply_rotation(images, angle=10):
    """Apply rotation perturbation (±angle degrees)"""
    rotated_images = np.zeros_like(images)

    for i in range(len(images)):
        img = images[i].squeeze()
        # Randomly choose rotation direction
        rot_angle = np.random.uniform(-angle, angle)
        rotated = ndimage.rotate(img, rot_angle, reshape=False, mode='nearest')
        rotated_images[i] = rotated.reshape(28, 28, 1)

    return rotated_images

def apply_occlusion(images, occlusion_size=0.2):
    """Apply occlusion perturbation (block out portion of image)"""
    occluded_images = np.copy(images)
    h, w = images.shape[1:3]
    block_h = int(h * occlusion_size)
    block_w = int(w * occlusion_size)

    for i in range(len(images)):
        # Random position for occlusion
        y_start = np.random.randint(0, h - block_h)
        x_start = np.random.randint(0, w - block_w)

        # Apply occlusion (set to black or random noise)
        occluded_images[i, y_start:y_start+block_h, x_start:x_start+block_w, :] = 0

    return occluded_images

def apply_gaussian_noise(images, noise_level=0.1):
    """Apply Gaussian noise perturbation"""
    noisy_images = np.copy(images)
    noise = np.random.normal(0, noise_level, images.shape)
    noisy_images = np.clip(noisy_images + noise, 0, 1)
    return noisy_images

# Create perturbed test datasets
print("\nCreating perturbed test datasets...")

# 1. Rotation perturbation (±10 degrees)
x_test_rotated = apply_rotation(x_test, angle=10)
print(f"Created rotated dataset: {x_test_rotated.shape}")

# 2. Occlusion perturbation (20% area)
x_test_occluded = apply_occlusion(x_test, occlusion_size=0.2)
print(f"Created occluded dataset: {x_test_occluded.shape}")

# 3. Gaussian noise perturbation
x_test_noisy = apply_gaussian_noise(x_test, noise_level=0.1)
print(f"Created noisy dataset: {x_test_noisy.shape}")

# Visualize perturbed images
fig, axes = plt.subplots(4, 5, figsize=(15, 12))

perturbation_types = ['Original', 'Rotated (±10°)', 'Occluded (20%)', 'Gaussian Noise']
datasets = [x_test, x_test_rotated, x_test_occluded, x_test_noisy]

for i, (pert_type, data) in enumerate(zip(perturbation_types, datasets)):
    for j in range(5):
        axes[i, j].imshow(data[j].squeeze(), cmap='gray')
        if j == 0:
            axes[i, j].set_ylabel(pert_type, rotation=90, size='large')
        axes[i, j].set_title(f"Label: {y_test[j]}")
        axes[i, j].axis('off')

plt.suptitle("Visualization of Different Perturbation Types", fontsize=16)
plt.tight_layout()
plt.show()

# Test models on perturbed datasets
print("\n" + "="*60)
print("TESTING MODELS ON PERTURBED DATASETS")
print("="*60)

# Create a results dictionary
perturbation_results = {}

# Test each model on each perturbation
for model_name, model in models.items():
    print(f"\nTesting {model_name.upper()} model:")

    # Get baseline accuracy on original test set
    _, baseline_acc = model.evaluate(x_test, y_test_categorical, verbose=0)

    # Test on rotated
    _, rot_acc = model.evaluate(x_test_rotated, y_test_categorical, verbose=0)

    # Test on occluded
    _, occ_acc = model.evaluate(x_test_occluded, y_test_categorical, verbose=0)

    # Test on noisy
    _, noise_acc = model.evaluate(x_test_noisy, y_test_categorical, verbose=0)

    perturbation_results[model_name] = {
        'baseline': baseline_acc,
        'rotated': rot_acc,
        'occluded': occ_acc,
        'noisy': noise_acc
    }

    print(f"  Baseline accuracy: {baseline_acc:.4f}")
    print(f"  Rotated accuracy:  {rot_acc:.4f} (Δ: {rot_acc - baseline_acc:+.4f})")
    print(f"  Occluded accuracy: {occ_acc:.4f} (Δ: {occ_acc - baseline_acc:+.4f})")
    print(f"  Noisy accuracy:    {noise_acc:.4f} (Δ: {noise_acc - baseline_acc:+.4f})")

# Create comparison table
print("\n" + "="*60)
print("PERTURBATION RESULTS SUMMARY TABLE")
print("="*60)

# Print table header
print(f"\n{'Model':<12} {'Baseline':<10} {'Rotated':<10} {'Occluded':<10} {'Noisy':<10} {'Worst Drop':<12}")
print("-" * 70)

# Print table rows
for model_name in models.keys():
    results = perturbation_results[model_name]
    worst_drop = min(results['rotated'] - results['baseline'],
                     results['occluded'] - results['baseline'],
                     results['noisy'] - results['baseline'])

    print(f"{model_name.upper():<12} {results['baseline']:.4f}     {results['rotated']:.4f}     "
          f"{results['occluded']:.4f}     {results['noisy']:.4f}     {worst_drop:+.4f}")

# Create bar chart comparison
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

model_names = list(models.keys())
x = np.arange(len(model_names))
width = 0.25

# Plot for each perturbation
perturbations = ['rotated', 'occluded', 'noisy']
perturbation_labels = ['Rotated', 'Occluded', 'Noisy']
colors = ['lightcoral', 'lightblue', 'lightgreen']

for i, (pert, label, color) in enumerate(zip(perturbations, perturbation_labels, colors)):
    accuracies = [perturbation_results[m][pert] for m in model_names]
    baseline_accs = [perturbation_results[m]['baseline'] for m in model_names]

    axes[i].bar(x - width/2, baseline_accs, width, label='Baseline', alpha=0.7)
    axes[i].bar(x + width/2, accuracies, width, label=label, alpha=0.7, color=color)

    axes[i].set_xlabel('Model')
    axes[i].set_ylabel('Accuracy')
    axes[i].set_title(f'{label} Perturbation Impact')
    axes[i].set_xticks(x)
    axes[i].set_xticklabels([m.upper() for m in model_names])
    axes[i].set_ylim([0.85, 1.0])
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

    # Add accuracy drop annotations
    for j, m in enumerate(model_names):
        drop = perturbation_results[m][pert] - perturbation_results[m]['baseline']
        axes[i].text(j, perturbation_results[m][pert] + 0.005, f'Δ{drop:+.3f}',
                    ha='center', va='bottom', fontsize=9)

plt.suptitle('Model Robustness to Different Perturbations', fontsize=16)
plt.tight_layout()
plt.show()

"""Step 8 Task 3: Visualizing the feature space with t-SNE and PCA"""

# Step 8: Task 3 - Feature Space Visualization and Analysis (Fixed)

print("="*60)
print("TASK 3: FEATURE SPACE VISUALIZATION")
print("="*60)

# First, let's build a simple feature extractor by recreating the model architecture
def build_feature_extractor():
    """Build a model that outputs features from the flatten layer"""
    model = keras.Sequential([
        layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (5, 5), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten()
    ])
    return model

# Create feature extractor and copy weights from the original model
print("\nBuilding feature extractor...")
feature_extractor = build_feature_extractor()

# We need to copy weights from the original model to our feature extractor
original_model = models['original']

# Find the corresponding layers and copy weights
for i in range(4):  # First 4 layers of original model (2 conv + 2 pooling)
    feature_extractor.layers[i].set_weights(original_model.layers[i].get_weights())

print("Feature extractor built successfully!")

# Extract features function
def extract_features_simple(model, images, num_samples=1000):
    """Extract features using the built feature extractor"""
    indices = np.random.choice(len(images), min(num_samples, len(images)), replace=False)
    sample_images = images[indices]
    sample_labels = y_test[indices]

    features = model.predict(sample_images, verbose=0)
    return features, sample_labels, indices

# Extract features for each dataset
print("\nExtracting features for visualization...")

original_features, original_labels, _ = extract_features_simple(feature_extractor, x_test, 1000)
rotated_features, rotated_labels, _ = extract_features_simple(feature_extractor, x_test_rotated, 1000)
occluded_features, occluded_labels, _ = extract_features_simple(feature_extractor, x_test_occluded, 1000)
noisy_features, noisy_labels, _ = extract_features_simple(feature_extractor, x_test_noisy, 1000)

print(f"Original features shape: {original_features.shape}")
print(f"Rotated features shape: {rotated_features.shape}")
print(f"Occluded features shape: {occluded_features.shape}")
print(f"Noisy features shape: {noisy_features.shape}")

# Apply PCA for dimensionality reduction
print("\nApplying PCA for dimensionality reduction...")
from sklearn.decomposition import PCA

# Combine all features for consistent PCA transformation
all_features = np.vstack([original_features, rotated_features, occluded_features, noisy_features])
pca = PCA(n_components=50)  # Reduce to 50 components first
all_features_pca = pca.fit_transform(all_features)

# Split back
n_samples = 1000
original_pca = all_features_pca[:n_samples]
rotated_pca = all_features_pca[n_samples:2*n_samples]
occluded_pca = all_features_pca[2*n_samples:3*n_samples]
noisy_pca = all_features_pca[3*n_samples:4*n_samples]

print(f"Explained variance by 50 PCA components: {pca.explained_variance_ratio_.sum():.4f}")

# Apply t-SNE on PCA-reduced features
print("\nApplying t-SNE for 2D visualization...")
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
all_features_tsne = tsne.fit_transform(all_features_pca)

# Split back
original_tsne = all_features_tsne[:n_samples]
rotated_tsne = all_features_tsne[n_samples:2*n_samples]
occluded_tsne = all_features_tsne[2*n_samples:3*n_samples]
noisy_tsne = all_features_tsne[3*n_samples:4*n_samples]

# Create visualization plots
print("\nCreating feature space visualizations...")

# Plot 1: t-SNE visualization of original data with labels
plt.figure(figsize=(15, 12))

# Plot original data with true labels
plt.subplot(2, 2, 1)
scatter = plt.scatter(original_tsne[:, 0], original_tsne[:, 1],
                      c=original_labels, cmap='tab10', alpha=0.6, s=20)
plt.colorbar(scatter, label='Digit Class')
plt.title('Original Test Data - t-SNE Visualization')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.grid(True, alpha=0.3)

# Plot 2: Comparison of original vs rotated
plt.subplot(2, 2, 2)
plt.scatter(original_tsne[:, 0], original_tsne[:, 1],
            c='blue', alpha=0.3, s=20, label='Original')
plt.scatter(rotated_tsne[:, 0], rotated_tsne[:, 1],
            c='red', alpha=0.3, s=20, label='Rotated')
plt.title('Original vs Rotated Features')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 3: Comparison of original vs occluded
plt.subplot(2, 2, 3)
plt.scatter(original_tsne[:, 0], original_tsne[:, 1],
            c='blue', alpha=0.3, s=20, label='Original')
plt.scatter(occluded_tsne[:, 0], occluded_tsne[:, 1],
            c='orange', alpha=0.3, s=20, label='Occluded')
plt.title('Original vs Occluded Features')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 4: Comparison of original vs noisy
plt.subplot(2, 2, 4)
plt.scatter(original_tsne[:, 0], original_tsne[:, 1],
            c='blue', alpha=0.3, s=20, label='Original')
plt.scatter(noisy_tsne[:, 0], noisy_tsne[:, 1],
            c='green', alpha=0.3, s=20, label='Noisy')
plt.title('Original vs Noisy Features')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend()
plt.grid(True, alpha=0.3)

plt.suptitle('Feature Space Analysis using t-SNE', fontsize=16)
plt.tight_layout()
plt.show()

# Create a final comprehensive analysis
print("\n" + "="*60)
print("FINAL PERTURBATION ANALYSIS SUMMARY")
print("="*60)

# Create summary table
print("\nPERFORMANCE SUMMARY TABLE:")
print("-" * 70)
print(f"{'Model':<12} {'Original':<10} {'Rotated':<10} {'Occluded':<10} {'Noisy':<10} {'Worst Drop':<12}")
print("-" * 70)

for model_name in ['original', 'dog', 'blur']:
    results = perturbation_results[model_name]
    worst_drop = min(results['rotated'] - results['baseline'],
                     results['occluded'] - results['baseline'],
                     results['noisy'] - results['baseline'])

    print(f"{model_name.upper():<12} {results['baseline']:.4f}     {results['rotated']:.4f}     "
          f"{results['occluded']:.4f}     {results['noisy']:.4f}     {worst_drop:+.4f}")

# Create final comparison visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Plot 1: Accuracy comparison across models and perturbations
model_names = ['ORIGINAL', 'DOG', 'BLUR']
perturbations = ['baseline', 'rotated', 'occluded', 'noisy']
pert_labels = ['Original', 'Rotated', 'Occluded', 'Noisy']

# Prepare data for grouped bar chart
x = np.arange(len(model_names))
width = 0.2

for i, (pert, label) in enumerate(zip(perturbations, pert_labels)):
    acc_values = [perturbation_results[m][pert] for m in ['original', 'dog', 'blur']]
    axes[0, 0].bar(x + i*width - width*1.5, acc_values, width, label=label, alpha=0.8)

axes[0, 0].set_xlabel('Model')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].set_title('Model Performance Under Different Perturbations')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(model_names)
axes[0, 0].set_ylim([0.94, 1.0])
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: Feature space compactness (using PCA variance)
# Calculate within-class variance for each perturbation
def calculate_within_class_variance(features, labels):
    """Calculate average within-class variance"""
    total_variance = 0
    for class_label in range(10):
        class_indices = np.where(labels == class_label)[0]
        if len(class_indices) > 1:
            class_features = features[class_indices]
            class_variance = np.var(class_features, axis=0).mean()
            total_variance += class_variance
    return total_variance / 10

variances = {
    'Original': calculate_within_class_variance(original_pca, original_labels),
    'Rotated': calculate_within_class_variance(rotated_pca, rotated_labels),
    'Occluded': calculate_within_class_variance(occluded_pca, occluded_labels),
    'Noisy': calculate_within_class_variance(noisy_pca, noisy_labels)
}

axes[0, 1].bar(variances.keys(), variances.values(),
               color=['blue', 'red', 'orange', 'green'])
axes[0, 1].set_title('Feature Space Compactness (Within-Class Variance)')
axes[0, 1].set_ylabel('Average Variance')
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: t-SNE of all perturbations (simplified)
all_tsne = np.vstack([original_tsne, rotated_tsne, occluded_tsne, noisy_tsne])
pert_labels_full = ['Original']*n_samples + ['Rotated']*n_samples + ['Occluded']*n_samples + ['Noisy']*n_samples

for pert, color in zip(['Original', 'Rotated', 'Occluded', 'Noisy'],
                       ['blue', 'red', 'orange', 'green']):
    mask = np.array(pert_labels_full) == pert
    axes[1, 0].scatter(all_tsne[mask, 0], all_tsne[mask, 1],
                       c=color, alpha=0.4, s=15, label=pert)
axes[1, 0].set_title('Feature Space Under Different Perturbations')
axes[1, 0].set_xlabel('t-SNE Dimension 1')
axes[1, 0].set_ylabel('t-SNE Dimension 2')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Plot 4: Summary text analysis
axes[1, 1].axis('off')

analysis_text = (
    "KEY FINDINGS - TASK 3:\n\n"
    "1. ACCURACY IMPACT:\n"
    "   • Occlusion: Largest accuracy drop\n"
    "   • Rotation: Moderate impact\n"
    "   • Noise: Minimal impact\n\n"

    "2. FEATURE STABILITY:\n"
    "   • Noise: Features remain clustered\n"
    "   • Rotation: Slight feature scattering\n"
    "   • Occlusion: Significant scattering\n\n"

    "3. MODEL COMPARISON:\n"
    "   • Original: Most robust overall\n"
    "   • DoG: Most sensitive to occlusion\n"
    "   • Blur: Balanced performance\n\n"

    "4. VISUAL SYSTEM ANALOGY:\n"
    "   • Similar to human vision:\n"
    "     - Occlusion disrupts most\n"
    "     - Rotation manageable\n"
    "     - Noise filtered well"
)

axes[1, 1].text(0.05, 0.5, analysis_text, fontsize=12,
                verticalalignment='center', family='monospace',
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))

plt.suptitle('Comprehensive Analysis: Visual System Robustness to Perturbations', fontsize=16)
plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("CONCLUSIONS AND INSIGHTS")
print("="*60)

print("\n1. MOST DESTRUCTIVE PERTURBATION:")
print("   • OCCLUSION (20% area)")
print("   • Reasons:")
print("     - Removes critical structural information")
print("     - Creates incomplete features")
print("     - Network cannot infer missing parts effectively")
print(f"   • Average accuracy drop: {(perturbation_results['original']['occluded'] - perturbation_results['original']['baseline']):.4f}")

print("\n2. LEAST DESTRUCTIVE PERTURBATION:")
print("   • GAUSSIAN NOISE")
print("   • Reasons:")
print("     - Network learns to filter noise")
print("     - Global structure preserved")
print("     - Similar to natural image variations")
print(f"   • Average accuracy drop: {(perturbation_results['original']['noisy'] - perturbation_results['original']['baseline']):.4f}")

print("\n3. PREPROCESSING IMPACT ON ROBUSTNESS:")
print("   • Original: Best overall robustness")
print("   • DoG Filter: More sensitive to perturbations")
print("   • Gaussian Blur: Intermediate robustness")

print("\n4. VISUAL SYSTEM ANALOGY:")
print("   • Similar to human visual cortex:")
print("     - Edge detection (DoG) enhances certain features")
print("     - Blurring reduces high-frequency noise")
print("     - Both preprocessing methods trade off between")
print("       discriminative power and robustness")

print("\n" + "="*60)
print("ASSIGNMENT COMPLETED SUCCESSFULLY!")
print("="*60)
print("\nAll three tasks have been completed:")
print("✓ Task 1: Preprocessing comparison and training")
print("✓ Task 2: Feature visualization and analysis")
print("✓ Task 3: Perturbation testing and robustness analysis")
print("\nResults are saved in the visualizations above.")

# Save all plots as images
import os
os.makedirs('results', exist_ok=True)

# Save key visualizations
for i, fig in enumerate(plt.get_fignums()):
    plt.figure(fig)
    plt.savefig(f'results/figure_{i}.png', dpi=150, bbox_inches='tight')